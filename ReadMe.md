# MTCMB Multi-Task TCM Evaluation Benchmark

<center>

![Python 3.12](https://img.shields.io/badge/Python-3.12-lightblue) ![Torch 2.3.1](https://img.shields.io/badge/PyTorch-2.3.1-lightblue) ![OpenAi 1.25.0](https://img.shields.io/badge/openai-1.25.0-lightblue) ![bert-score](https://img.shields.io/badge/bert--score-0.3.13-lightblue)
</center>

![title](https://github.com/Wayyuanyuan/MTCMB/blob/main/pics/title2.png)

<p align="center">
   ğŸ“ƒ <a href="" target="_blank">Paper</a> â€¢ ğŸŒ <a href="" target="_blank">Website</a>  
   <br>  <a href="https://github.com/Wayyuanyuan/MTCMB/blob/main/ReadMe_cn.md">   ä¸­æ–‡</a> | <a href="https://github.com/Wayyuanyuan/MTCMB/blob/main/ReadMe.md"> English
</p>




## ğŸŒˆ Update

- **[2025.5.15]** The research article has been released.
- **[2025.5.15]** ğŸ‰ğŸ‰ğŸ‰The official release of the MTCMB guidelines is hereby announced!ğŸ‰ğŸ‰ğŸ‰

## ğŸŒ Data Download

ï¼ˆ1ï¼‰ZIP format

```python
  https://github.com/Wayyuanyuan/MTCMB.git && cd data
```

ï¼ˆ2ï¼‰[Baidu Netdisk Link](https://pan.baidu.com/s/1_pOlvjRNEbOp29oDPi7bRQ?pwd=vgzt)



## ğŸ“Ranking List

## ğŸ˜ŠDataset Description

![pie-nest](https://github.com/Wayyuanyuan/CTCMB/blob/main/pics/line-simple-en.png)

#### **Data Structure**

Dataset: **5** Dimensions, **12** Datasets

The following figure shows the data volume distribution in four dimensions: language understanding, diagnosis, prescription recommendation, and safety assessment:

![pie-nest](https://github.com/Wayyuanyuan/MTCMB/blob/main/pics/area-stack%20-en.png)

ğŸ¥¸ **Knowledge Question Answering Dimension** contains three data sets, namely TCM-ED-A (1200), TCM-ED-B (4800), TCM-FT (100)

- **Knowledge Question and Answer**: Test the LLM's understanding and application of core knowledge in basic theories of TCM, formula science, acupuncture, and diagnostics through questions from the Intermediate Attending Physician and Licensed Physician Examinations, as well as standard essay-style questions.

- **Language Understanding**: This dimension evaluates the performance of the LLM in TCM text understanding and information extraction through entity extraction in medical records, generating structured medical records from doctor-patient conversations, and answering questions based on literature content.

- **Diagnosis**: This dimension examines the ability of the LLM to conduct syndrome differentiation analysis based on the patient's clinical information (such as symptoms, signs, tongue and pulse, etc.) and accurately determine the name of the disease and syndrome type.

- **Prescription Recommendation**: This dimension assesses LLM's ability to recommend appropriate prescriptions based on the description of the disease and the characteristics of the syndrome. It specifically covers the understanding of the composition of the prescription, the rules of compatibility, and the correspondence between symptoms and diseases.

- **Safety evaluation**: This dimension evaluates the ability of the LLM to identify safety risks involved in TCM practice through fill-in-the-blank questions and multiple-choice questions.

####  Detailed Information

**By clicking the hyperlink, you can access the format requirements for various datasets.**

| Dimension                     | Dataset name                                                 | Quantity | Task description                                             | Data source                                                  | Construction method                                          | Evaluation method                                 |
| ----------------------------- | ------------------------------------------------------------ | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------- |
| Knowledge Question and Answer | [TCM-ED-A](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-ED-A.md) | 1,200    | Single-choice questions                                      | 12 subjects of the TCM intermediate attending physician examination | 100 questions are randomly selected for each subject         | Accuracy                                          |
|                               | [TCM-ED-B](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-ED-B.md) | 4800     | Single-choice questions                                      | Practicing physician question bank                           | 8 complete practicing physician examination papers           | Accuracy                                          |
|                               | [TCM-FT](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-FT.md) | 100      | Questions and Answers                                        | "Chinese Medicine Question and Answer Database" edited by Hu Ximing | 100 questions and answers are randomly selected from the question bank and reviewed by professionals | BertScore                                         |
| Language Understanding        | [TCMeEE](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCMeEE.md) | 100      | Based on medical records, entities related to Chinese medicine are identified and extracted to generate structured medical records. | The medical cases are from the website [ã€ŠTCM Think Tankã€‹](https://zhongyigen.com/) and real medical cases provided by Hunan University of Chinese Medicine | Use deepseek-r1 to generate answers, and then professionals review the generated answers | The average of BERTScore, ROUGE and BLEU is taken |
|                               | [TCM-CHGD](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-CHGD.md) | 100      | Generate medical cases based on doctor-patient dialogues.    | Call deepseek r1 to generate doctor-patient dialogues based on real medical cases | 100 medical cases are used to reversely generate doctor-patient dialogues | The average of BERTScore, ROUGE and BLEU is taken |
|                               | [TCM-LitData](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-LitData.md) | 100      | Answer questions based on the content of the literature.     | [Dataset for TCM Literature Question Generation from Alibaba Cloud Tianchi Lab](https://tianchi.aliyun.com/dataset/86895) | 100 questions are randomly selected from the dataset and reviewed by professionals | Average of ROUGE and BLEU                         |
| Diagnosis                     | [TCM-MSDD](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-MSDD.md) | 100      | Infer the corresponding syndrome type and disease name from clinical information. | [Alibaba Cloud Tianchi Lab CCL25-Eval Task 9 Dataset Subtask 1](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-MSDD) | Randomly select 100 questions and have them reviewed by professionals | CCL25-Eval Task 9 task1_score                     |
|                               | [TCM-Diagnosis](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-Diagnosis.md) | 200      | Give the disease name, syndrome name, location, and nature of the disease based on the symptoms. | Real internal medicine, internal medicine, gynecology and pediatrics syndrome data set provided by Hunan University of Chinese Medicine | 50 cases are selected from each of the four subjects of internal medicine, internal medicine, gynecology and pediatrics | Average of BERTScore, ROUGE and BLEU              |
| Prescription recommendation   | [TCM-PR](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-PR.md) | 100      | Recommend appropriate Chinese medicine prescriptions based on clinical information. | [Alibaba Cloud Tianchi Laboratory CCL25-Eval Task 9 Dataset Subtask 2]() | Randomly select 100 questions from the dataset and review them by professionals | CCL25-Eval Task 9 task2_score                     |
|                               | [TCM-FRD](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-FRD.md) | 200      | Give treatment methods, prescription names, and drug composition (excluding dosage) based on the manifestation of the syndrome. | Real internal medicine, internal medicine, gynecology and pediatric syndrome data set provided by Hunan University of Chinese Medicine | 200 cases of internal medicine, internal medicine, gynecology and pediatrics were selected | Average of BERTScore, ROUGE and BLEU              |
| Safety evaluation             | [TCM-SAFE1](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-SAFE1%262.md) | 50       | Fill-in-the-blank questions                                  | Safety problem data set provided by Hunan University of Chinese Medicine | Common Chinese medicine and acupuncture contraindications fill-in-the-blank questions (50 questions) | LLM scoring                                       |
|                               | [TCM-SAFE2](https://github.com/Wayyuanyuan/MTCMB/blob/main/dataset_info/TCM-SAFE1%262.md) | 50       | Single-choice questions                                      | Safety problem data set provided by Hunan University of Chinese Medicine | Common contraindications of traditional Chinese medicine and acupuncture (single-choice questions) (50 questions) | Accuracy                                          |



## ğŸ”†Submission and Evaluation Process

### Environment Configuration


Ensure that your development environment has installed the Python libraries required by the [requirements.txt file](https://github.com/Wayyuanyuan/MTCMB/blob/main/requirements.txt).


Currently, we provide an invocation template based on the OpenAI library, as well as three invocation templates for open-source libraries on Hugging Face: `HuatuoGPT-II`, `Taiyi-LLM`, and `WiNGPT2`. If you need support for additional invocations, please extend the `ChatInvoker` interface from the `make_answer/chat/chat_invoker.py` module.

#### OpenAI Library-Based Invocation Template

`module_name.call_llm`.py


```python
import os
import openai

from loguru import logger
from make_answer.chat.chat_invoker import ChatInvoker


class LlmOpenai(ChatInvoker):
    def __init__(self, *args, **kwargs):
        base_url = os.environ.get("OPENAI_BASE_URL")
        if "base_url" in kwargs:
            base_url = kwargs["base_url"]
        api_key = os.environ.get("OPENAI_API_KEY")
        if "api_key" in kwargs:
            api_key = kwargs["api_key"]
        self.client = openai.OpenAI(
            base_url=base_url, api_key=api_key)
        self.model_name = kwargs["model_name"]

    def chat(self, msg: str, *args, **kwargs) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šä¸­åŒ»åŒ»ç”Ÿï¼Œèƒ½å¤Ÿå‡†ç¡®å…¨é¢çš„è§£ç­”ä¸­åŒ»é—®é¢˜ã€‚æœ¬æ¬¡å¯¹è¯ï¼Œå‡åªé‡‡ç”¨ä¸­æ–‡æé—®å’Œå›ç­”ã€‚"},
                {"role": "user", "content": msg}
            ]
        )
        try:
            ret = response.choices[0].message.content
        except Exception as e:
            logger.exception(f"call openai chat api error: {response}")
            raise e

        return ret
```

##### Usage

```python
python main.py \
--step-chat data/ \  # Folder containing the test questions
--api-model module_name.call_llm_file.ClassName \  # Custom test model, must inherit from ChatInvoker; provide the full module name, file name, and class name
--api-model-name large_model_name \  # Name of the large model being invoked, used to differentiate between different models and their results
--base-url model_api_url \  # URL for invoking the model
--api-key model_api_key  # API key for invoking the model
```

##### Example of Invocation Using the OpenAI Library

```
python main.py --step-chat data --api-model make_answer.chat.remote.openai_api.LlmOpenai --llm-name your_model_name  --base-url your_url --api-key your_key --num-process 12
```



#### Local Invocation Method

`module_name.call_llm`.py

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from make_answer.chat.chat_invoker import ChatInvoker


class LocalLLM(ChatInvoker):
    def __init__(self, model_path: str, gpu_id: int = 0):
        # Model initialization, which is only performed on the first run.

    def chat(
            self, msg: str, *args, **kwargs
    ) -> str:
        # To request a response from the model, the msg parameter is required.
```

##### Usage Instructions

```python
python main.py \
--step-chat data/ \  # Directory containing the test questions
--local-model /Path/To/LLM \  # Directory where the local large language model (LLM) is located
--model-type LLM_name  # Name of the large language model; the custom template constructor must be written in `make_answer/chat/__init__.py` within the `name_model_dict`.
```

##### Example

```
python main.py --step-chat data/ --local-model  /mnt/data1/MedLLM_baselines/Taiyi --model-type taiyi
```



## Submission Format Requirements



Our evaluation benchmark includes 11 datasets, each containing multiple questions with an empty 'answer' field. A brief example is provided below:

```
{"question": "ä½•è°“è¡€ç˜€?è¡€ç˜€æ˜¯å¦‚ä½•å½¢æˆçš„?", "answer": ""}
{"question": "è‚¾â€œå…¶ååœ¨å‘â€æœ‰ä½•ç†è®ºä¾æ®?", "answer": ""}
{"question": "å¥‡ç»å…«è„‰æœ‰ä½•ä¸»è¦ç”Ÿç†åŠŸèƒ½?", "answer": ""}
```

Our Evaluation Code Requires the Following Data Format

```
{"question": "ä½•è°“è¡€ç˜€?è¡€ç˜€æ˜¯å¦‚ä½•å½¢æˆçš„?", "answer": "è¡€ç˜€æ˜¯æŒ‡è¡€æ¶²è¿è¡Œä¸ç•…ã€åœæ»äºè¡€è„‰ä¹‹ä¸­è€Œå½¢æˆçš„ä¸€ç§ç—…ç†çŠ¶æ€ã€‚å…¶ä¸»è¦è¡¨ç°ä¸ºçš®è‚¤å‡ºç°é’ç´«æ–‘å—ã€ç–¼ç—›ç­‰ç—‡çŠ¶ã€‚è¡€ç˜€çš„å½¢æˆåŸå› åŒ…æ‹¬å¤–ä¼¤ã€æƒ…ç»ªæ³¢åŠ¨ç­‰å› ç´ ï¼Œä¹Ÿä¸ä½“å†…æ°”è™šã€é˜³è™šç­‰å†…åœ¨å› ç´ æœ‰å…³ã€‚æ²»ç–—ä¸Šåº”ä»¥æ´»è¡€åŒ–ç˜€ä¸ºä¸»ï¼Œå¯é‡‡ç”¨é’ˆç¸ã€ä¸­è¯ç­‰æ–¹å¼è¿›è¡Œè°ƒç†ã€‚"}
{"question": "è‚¾â€œå…¶ååœ¨å‘â€æœ‰ä½•ç†è®ºä¾æ®?", "answer": "ã€Šé»„å¸å†…ç»ã€‹ä¸­è®°è½½ï¼šâ€œäººä¹‹äº”è„å…­è…‘ã€å››è‚¢ç™¾éª¸çš†ç¦€äºè‡ªç„¶ï¼Œè€Œè¡€æ°”äº¦å„æœ‰æ‰€ä¸»ã€‚å¿ƒè—ç¥ï¼Œè‚ºè—é­„ï¼Œè‚è—é­‚ï¼Œè„¾è—æ„ï¼Œè‚¾è—å¿—ï¼Œæ­¤äº”è€…ï¼Œçš†è—äºç²¾ã€‚â€å…¶ä¸­ï¼Œâ€œè‚¾è—å¿—â€ï¼Œå³æŒ‡è‚¾è„æ‰€è—ä¹‹ç²¾ç¥æ„å¿—ã€‚â€œå…¶ååœ¨å‘â€ï¼Œåˆ™è¯´æ˜è‚¾è„çš„ç²¾ç¥çŠ¶æ€åæ˜ åœ¨å¤–è¡¨ä¸Šï¼Œå³å¤´å‘çš„çŠ¶æ€ã€‚å› æ­¤ï¼Œä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œè‚¾â€œå…¶ååœ¨å‘â€çš„ç†è®ºä¾æ®æ˜¯ã€Šé»„å¸å†…ç»ã€‹ä¸­çš„â€œäº”è„å…­è…‘ã€å››è‚¢ç™¾éª¸çš†ç¦€äºè‡ªç„¶ï¼Œè€Œè¡€æ°”äº¦å„æœ‰æ‰€ä¸»ã€‚å¿ƒè—ç¥ï¼Œè‚ºè—é­„ï¼Œè‚è—é­‚ï¼Œè„¾è—æ„ï¼Œè‚¾è—å¿—ï¼Œæ­¤äº”è€…ï¼Œçš†è—äºç²¾ã€‚â€è¿™ä¸€ç†è®ºè®¤ä¸ºï¼Œäººä½“çš„å„ç§ç”Ÿç†åŠŸèƒ½éƒ½ä¸äº”è„æœ‰å…³ï¼Œè€Œå¤´å‘çš„çŠ¶æ€åˆ™æ˜¯ç”±è‚¾è„æ‰€æ”¯é…çš„ã€‚å› æ­¤ï¼Œè‚¾è„çš„ç²¾ç¥çŠ¶æ€å¯ä»¥é€šè¿‡å¤´å‘æ¥è¡¨ç°å‡ºæ¥ã€‚"}
{"question": "å¥‡ç»å…«è„‰æœ‰ä½•ä¸»è¦ç”Ÿç†åŠŸèƒ½?", "answer": "å¥‡ç»å…«è„‰æ˜¯ä¸­åŒ»å­¦ä¸­çš„é‡è¦ç†è®ºä¹‹ä¸€ï¼Œå…¶ä¸»è¦ç”Ÿç†åŠŸèƒ½åŒ…æ‹¬è°ƒèŠ‚å…¨èº«æ°”è¡€è¿è¡Œã€ç»´æŒè„è…‘åŠŸèƒ½å¹³è¡¡ã€ä¿ƒè¿›äººä½“é˜´é˜³åè°ƒç­‰æ–¹é¢ã€‚å…¶ä¸­ï¼Œä»»ç£äºŒè„‰æ˜¯å¥‡ç»å…«è„‰çš„æ ¸å¿ƒï¼Œé€šè¿‡è°ƒèŠ‚å¿ƒè‚¾ä¹‹é—´çš„æ°´ç«ç›¸æµå…³ç³»ï¼Œè¾¾åˆ°è°ƒå’Œé˜´é˜³ã€å¹³è¡¡æ°”è¡€çš„ç›®çš„ï¼›è€Œå†²ä»»äºŒè„‰åˆ™ä¸å¥³æ€§ç”Ÿæ®–ç³»ç»Ÿå¯†åˆ‡ç›¸å…³ï¼Œå¯ä»¥è°ƒèŠ‚æœˆç»ã€å­•è‚²èƒå„¿ç­‰ç”Ÿç†è¿‡ç¨‹ï¼›å¸¦è„‰åˆ™æ˜¯äººä½“è…¹éƒ¨çš„ä¸€æ¡æ¨ªè¡Œè„‰ç»œï¼Œå…·æœ‰å›ºæŠ¤è…°è…¹ã€è°ƒèŠ‚è„¾èƒƒç­‰åŠŸèƒ½ï¼›é˜´ç»´è„‰åˆ™ä¸ºäººä½“é˜´æ°”çš„ä¸»è¦é€šé“ï¼Œå¯ä»¥è°ƒèŠ‚äººä½“é˜´æ¶²çš„ï¼Œç»´æŒäººä½“é˜´æ¶²çš„æ­£å¸¸ä»£è°¢ï¼›é˜³ç»´è„‰åˆ™ä¸ºäººä½“é˜³æ°”çš„ä¸»è¦é€šé“ï¼Œå¯ä»¥è°ƒèŠ‚äººä½“é˜³æ°”çš„æ­£å¸¸ä»£è°¢ã€‚æ€»ä¹‹ï¼Œå¥‡ç»å…«è„‰åœ¨äººä½“å†…å‘æŒ¥ç€é‡è¦çš„ç”Ÿç†ä½œç”¨ï¼Œå¯¹äººä½“å¥åº·æœ‰ç€ä¸å¯å¿½è§†çš„å½±å“ã€‚"}
```

Essentially, the 'answer' Field Is Populated with Model Responses. These responses are saved in a file named `mid.jsonl`.

Submission Naming Convention for 11 Datasets. We have 11 datasets, and we expect submissions to follow the naming convention outlined below:

```
Primary Directory (Model Name)
â”œâ”€â”€ TCM-ED-A
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-ED-B
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-FT
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCMeEE
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-CHGD
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-LitData
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-MSDD
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-Diagnosis
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-PR
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-FRD
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-SAFE1
â”‚   â””â”€â”€ mid.jsonl
â”œâ”€â”€ TCM-SAFE2
   â””â”€â”€ mid.jsonl
```

Please submit your files in a compressed package, such as **gemini-1.5-pro.zip**, and send it to **weiyy53@mail2.sysu.edu.cn**. After the evaluation process is complete, the results will be published on **GitHub**.  

If you have any questions, feel free to contact us.
## Acknowledgments

We sincerely thank all organizations and individuals who have provided support and assistance to this project. ğŸ‰ğŸ‰ğŸ‰  

We also extend our heartfelt gratitude to all team members who contributed to this project!

